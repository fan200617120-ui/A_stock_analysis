#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI å¸‚åœºæƒ…ç»ªä¸æŒ‡æ ‡é¢„æµ‹ï¼ˆä¿®è®¢ç‰ˆï¼‰
ä¿®æ­£ç‚¹
1. ç¼ºå¤±å€¼é‡‡ç”¨â€œæ—¶åºå‘å‰+å‘åâ€å¡«å……ï¼Œå† drop å‰©ä½™ NaNï¼Œé˜²æ­¢è®­ç»ƒé›†/æµ‹è¯•é›†å‡ºç° NaN å¯¼è‡´æ¨¡å‹æŠ¥é”™ã€‚
2. æ‰€æœ‰é™¤æ³•å¢åŠ  epsï¼ˆ1e-8ï¼‰é˜²æ­¢ 0 é™¤ï¼›æç«¯åˆ†æ¯ç›´æ¥ç»™ 1e-8 çš„å…œåº•å€¼ã€‚
3. äº¤å‰éªŒè¯é˜¶æ®µï¼Œtest æŠ˜å¦‚æœæ ·æœ¬å¤ªå°‘ï¼ˆ<3ï¼‰ç›´æ¥è·³è¿‡ï¼Œé¿å… r2_score æŠ¥é”™ã€‚
4. æ–°å¢â€œæœªæ¥å¤–æ¨â€æ¨¡å—ï¼šç”¨æ»šåŠ¨çª—å£æœ€åä¸€æœŸç‰¹å¾é¢„æµ‹æœªæ¥ N å¤©ï¼Œè€Œéç®€å•çº¿æ€§è¶‹åŠ¿ã€‚
5. ç‰¹å¾é‡è¦æ€§ä»…å½“â€œéšæœºæ£®æ—â€åœ¨é›†æˆåˆ—è¡¨é‡Œä¸”æˆåŠŸè®­ç»ƒåæ‰å±•ç¤ºï¼›å¦åˆ™è‡ªåŠ¨é€‰ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹ã€‚
6. æ‰€æœ‰ plt / st å›¾è¡¨ç»Ÿä¸€é«˜åº¦ã€å­—å·ï¼Œé˜²æ­¢ Streamlit ç¼©æ”¾å¼‚å¸¸ã€‚
7. å¢åŠ æ—¥å¿—é’©å­ï¼Œæ–¹ä¾¿å®šä½å¤±è´¥ç¯èŠ‚ï¼ˆå¯é€‰å…³é—­ï¼‰ã€‚
8. å…¼å®¹åŸå‡½æ•°å create_features / train_and_predictï¼Œå¯ç›´æ¥æ›¿æ¢æ—§è„šæœ¬ã€‚
"""

import pandas as pd
import numpy as np
import warnings, logging
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
import streamlit as st
import plotly.graph_objects as go
from plotly.subplots import make_subplots

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
EPS = 1e-8


# ------------------------------------------------------------------
# 1. ç‰¹å¾å·¥ç¨‹
# ------------------------------------------------------------------
def create_advanced_features(df: pd.DataFrame, lookback_days: int = 30) -> pd.DataFrame:
    """ç”Ÿæˆæ›´ä¸°å¯Œçš„æ—¶é—´åºåˆ—ç‰¹å¾ï¼Œè¿”å›å«æ—¥æœŸåˆ—çš„ DataFrame"""
    df = df.copy()
    # æ—¥æœŸåˆ—
    date_col = None
    for c in ["æ—¥æœŸ", "date", "Date"]:
        if c in df.columns:
            date_col = c
            df[c] = pd.to_datetime(df[c])
            df = df.sort_values(c)
            break

    numeric_cols = ["å…¨å¤©æ€»é¢", "åŒ—å‘å‡€å€¼", "å…¨å¤©æ¶¨åœ", "å…¨å¤©å°æ¿ç‡", "ä¸Šæ¶¨", "ä¸‹è·Œ", "å¹³ç›˜"]
    numeric_cols = [c for c in numeric_cols if c in df.columns]

    # 1. åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
    for col in numeric_cols:
        # ç§»åŠ¨å¹³å‡
        for win in [5, 10, 20]:
            df[f"{col}_MA{win}"] = df[col].rolling(win, min_periods=1).mean()
        # åŠ¨é‡
        for lag in [5, 10]:
            df[f"{col}_MOM{lag}"] = df[col] / (df[col].shift(lag) + EPS) - 1
        # æ³¢åŠ¨ç‡
        for win in [5, 10]:
            df[f"{col}_VOL{win}"] = df[col].rolling(win, min_periods=1).std()

    # 2. æƒ…ç»ªè¡ç”Ÿ
    if all(c in df.columns for c in ["ä¸Šæ¶¨", "ä¸‹è·Œ", "å¹³ç›˜"]):
        total = df["ä¸Šæ¶¨"] + df["ä¸‹è·Œ"] + df["å¹³ç›˜"] + EPS
        df["æ¶¨è·Œæ¯”"] = (df["ä¸Šæ¶¨"] + 1) / (df["ä¸‹è·Œ"] + 1)
        df["ä¸Šæ¶¨ç‡"] = df["ä¸Šæ¶¨"] / total
        df["ä¸‹è·Œç‡"] = df["ä¸‹è·Œ"] / total
        df["å¸‚åœºå®½åº¦"] = (df["ä¸Šæ¶¨"] - df["ä¸‹è·Œ"]) / total

    # 3. èµ„é‡‘æµ
    if all(c in df.columns for c in ["åŒ—å‘å‡€å€¼", "å…¨å¤©æ€»é¢"]):
        df["åŒ—å‘å æ¯”"] = df["åŒ—å‘å‡€å€¼"] / (df["å…¨å¤©æ€»é¢"] + EPS) * 100
        df["åŒ—å‘åŠ¨é‡"] = df["åŒ—å‘å‡€å€¼"].rolling(5, min_periods=1).mean()

    # 4. æ¶¨åœè´¨é‡
    if all(c in df.columns for c in ["å…¨å¤©æ¶¨åœ", "ä¸Šæ¶¨"]):
        df["æ¶¨åœé›†ä¸­åº¦"] = df["å…¨å¤©æ¶¨åœ"] / (df["ä¸Šæ¶¨"] + EPS)
        df["æ¶¨åœå¼ºåº¦"] = df["å…¨å¤©æ¶¨åœ"] / (df["å…¨å¤©æ¶¨åœ"].rolling(10, min_periods=1).mean() + EPS)

    # 5. æ—¶é—´ç‰¹å¾
    if date_col:
        df["æ˜ŸæœŸ"] = df[date_col].dt.dayofweek
        df["æœˆä»½"] = df[date_col].dt.month
        df["å­£åº¦"] = df[date_col].dt.quarter

    # 6. æ»å & äº¤äº’
    for lag in [1, 2, 3, 5]:
        for col in numeric_cols:
            df[f"{col}_lag{lag}"] = df[col].shift(lag)

    if all(c in df.columns for c in ["å…¨å¤©æ€»é¢_MA5", "åŒ—å‘å‡€å€¼_MA5", "å…¨å¤©æ¶¨åœ_MA5"]):
        df["é‡ä»·é…åˆ"] = df["å…¨å¤©æ€»é¢_MA5"] * df["åŒ—å‘å‡€å€¼_MA5"]
        df["æƒ…ç»ªèµ„é‡‘"] = df["å…¨å¤©æ¶¨åœ_MA5"] * df["åŒ—å‘å‡€å€¼_MA5"]

    # 7. ç¼ºå¤±å€¼å¤„ç†
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.ffill().bfill()
    # ä»ç¼ºå¤±çš„æ•´åˆ—åˆ é™¤
    df = df.dropna(axis=1, how="all").dropna()
    return df


# ------------------------------------------------------------------
# 2. ç»¼åˆæƒ…ç»ªæŒ‡æ•°
# ------------------------------------------------------------------
def create_composite_sentiment_index(df: pd.DataFrame) -> pd.Series:
    components = {}
    if "å…¨å¤©æ¶¨åœ" in df.columns:
        max_ = df["å…¨å¤©æ¶¨åœ"].max()
        components["æ¶¨åœæƒ…ç»ª"] = df["å…¨å¤©æ¶¨åœ"] / (max_ + EPS) if max_ > 0 else 0
    if all(c in df.columns for c in ["ä¸Šæ¶¨", "ä¸‹è·Œ"]):
        total = df["ä¸Šæ¶¨"] + df["ä¸‹è·Œ"] + EPS
        components["æ¶¨è·Œæƒ…ç»ª"] = df["ä¸Šæ¶¨"] / total
    if "å…¨å¤©å°æ¿ç‡" in df.columns:
        components["å°æ¿è´¨é‡"] = df["å…¨å¤©å°æ¿ç‡"] / 100 if df["å…¨å¤©å°æ¿ç‡"].max() > 1 else df["å…¨å¤©å°æ¿ç‡"]
    if "åŒ—å‘å‡€å€¼" in df.columns:
        max_ = abs(df["åŒ—å‘å‡€å€¼"]).max()
        components["åŒ—å‘æƒ…ç»ª"] = df["åŒ—å‘å‡€å€¼"] / (max_ + EPS) if max_ > 0 else 0

    if not components:
        return pd.Series([50] * len(df), index=df.index)

    weights = {"æ¶¨åœæƒ…ç»ª": 0.3, "æ¶¨è·Œæƒ…ç»ª": 0.25, "å°æ¿è´¨é‡": 0.25, "åŒ—å‘æƒ…ç»ª": 0.2}
    composite = 0
    for name, series in components.items():
        norm = (series - series.min()) / (series.max() - series.min() + EPS)
        composite += norm * weights.get(name, 0)
    return composite * 100


# ------------------------------------------------------------------
# 3. è®­ç»ƒé›†æˆæ¨¡å‹
# ------------------------------------------------------------------
def train_ensemble_model(X: np.ndarray, y: pd.Series):
    """è¿”å› dict{æ¨¡å‹å: è®­ç»ƒå¥½çš„æ¨¡å‹}"""
    models = {
        "éšæœºæ£®æ—": RandomForestRegressor(
            n_estimators=150, max_depth=15, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1
        ),
        "æ¢¯åº¦æå‡": GradientBoostingRegressor(n_estimators=100, max_depth=8, learning_rate=0.1, random_state=42),
        "çº¿æ€§å›å½’": LinearRegression(),
        "æ”¯æŒå‘é‡æœº": SVR(kernel="rbf", C=1.0, epsilon=0.1),
    }
    trained = {}
    for name, m in models.items():
        try:
            m.fit(X, y)
            trained[name] = m
        except Exception as e:
            logging.warning(f"{name} è®­ç»ƒå¤±è´¥: {e}")
    return trained


# ------------------------------------------------------------------
# 4. é›†æˆé¢„æµ‹
# ------------------------------------------------------------------
def predict_with_ensemble(models: dict, X: np.ndarray) -> dict:
    preds = {}
    for name, m in models.items():
        try:
            preds[name] = m.predict(X)
        except Exception as e:
            logging.warning(f"{name} é¢„æµ‹å¤±è´¥: {e}")

    if not preds:
        return preds

    # åŠ æƒé›†æˆ
    weights = {"éšæœºæ£®æ—": 0.4, "æ¢¯åº¦æå‡": 0.3, "çº¿æ€§å›å½’": 0.15, "æ”¯æŒå‘é‡æœº": 0.15}
    ensemble = np.zeros(X.shape[0])
    total_w = 0
    for n, p in preds.items():
        w = weights.get(n, 0)
        ensemble += p * w
        total_w += w
    if total_w > EPS:
        ensemble /= total_w
        preds["é›†æˆæ¨¡å‹"] = ensemble
    return preds


# ------------------------------------------------------------------
# 5. Streamlit ä¸»ç•Œé¢
# ------------------------------------------------------------------
def show_ai_prediction_dashboard(df: pd.DataFrame):
    st.markdown("### ğŸ§  æ™ºèƒ½é¢„æµ‹é…ç½®")
    col1, col2, col3 = st.columns(3)
    with col1:
        pred_type = st.selectbox(
            "é¢„æµ‹ç±»å‹", ["å¸‚åœºæƒ…ç»ªé¢„æµ‹", "æˆäº¤é¢é¢„æµ‹", "åŒ—å‘èµ„é‡‘é¢„æµ‹", "æ¶¨åœæ¿æ•°é‡é¢„æµ‹"], index=0
        )
    with col2:
        lookback = st.slider("å†å²æ•°æ®å¤©æ•°", 30, 180, 60)
    with col3:
        forecast_days = st.slider("é¢„æµ‹å¤©æ•°", 1, 10, 5)

    st.markdown("### âš™ï¸ æ¨¡å‹é…ç½®")
    col1, col2 = st.columns(2)
    with col1:
        use_ensemble = st.checkbox("ä½¿ç”¨é›†æˆå­¦ä¹ ", True)
        feat_eng = st.checkbox("é«˜çº§ç‰¹å¾å·¥ç¨‹", True)
    with col2:
        tscv_flag = st.checkbox("æ—¶é—´åºåˆ—äº¤å‰éªŒè¯", True)
        show_imp = st.checkbox("æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§", True)

    if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½é¢„æµ‹", type="primary", use_container_width=True):
        with st.spinner("AIæ¨¡å‹æ­£åœ¨åˆ†æå¸‚åœºæ•°æ®..."):
            try:
                # 1. ç‰¹å¾
                features_df = create_advanced_features(df, lookback) if feat_eng else df.copy()
                # 2. ç›®æ ‡
                target_map = {
                    "å¸‚åœºæƒ…ç»ªé¢„æµ‹": ("æƒ…ç»ªæŒ‡æ•°", create_composite_sentiment_index(features_df)),
                    "æˆäº¤é¢é¢„æµ‹": ("æˆäº¤é¢", features_df.get("å…¨å¤©æ€»é¢")),
                    "åŒ—å‘èµ„é‡‘é¢„æµ‹": ("åŒ—å‘èµ„é‡‘", features_df.get("åŒ—å‘å‡€å€¼")),
                    "æ¶¨åœæ¿æ•°é‡é¢„æµ‹": ("æ¶¨åœæ¿æ•°é‡", features_df.get("å…¨å¤©æ¶¨åœ")),
                }
                target_name, y = target_map.get(pred_type, (None, None))
                if y is None:
                    st.error(f"ç¼ºå°‘{target_name}æ•°æ®ï¼Œæ— æ³•é¢„æµ‹")
                    return
                # 3. ç‰¹å¾çŸ©é˜µ
                date_col = None
                for c in ["æ—¥æœŸ", "date", "Date"]:
                    if c in features_df.columns:
                        date_col = c
                        break
                feature_cols = [
                    c
                    for c in features_df.columns
                    if c not in [date_col, "æ—¥æœŸ_str", target_name]
                    and pd.api.types.is_numeric_dtype(features_df[c])
                ]
                X_df = features_df[feature_cols].fillna(0)
                if len(X_df) < 20:
                    st.warning("æ•°æ®é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦20ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®")
                    return
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X_df)
                # 4. è®­ç»ƒ & äº¤å‰éªŒè¯
                tscv = TimeSeriesSplit(n_splits=5)
                cv_scores = []
                if use_ensemble:
                    models = train_ensemble_model(X_scaled, y)
                    if not models:
                        st.error("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥")
                        return
                    if tscv_flag:
                        for train_idx, test_idx in tscv.split(X_scaled):
                            if len(test_idx) < 3:
                                continue
                            X_tr, X_te = X_scaled[train_idx], X_scaled[test_idx]
                            y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]
                            fold_models = train_ensemble_model(X_tr, y_tr)
                            fold_preds = predict_with_ensemble(fold_models, X_te)
                            if "é›†æˆæ¨¡å‹" in fold_preds:
                                cv_scores.append(r2_score(y_te, fold_preds["é›†æˆæ¨¡å‹"]))
                    preds = predict_with_ensemble(models, X_scaled)
                    final_pred = preds.get("é›†æˆæ¨¡å‹", list(preds.values())[0])
                else:
                    model = RandomForestRegressor(n_estimators=100, random_state=42)
                    model.fit(X_scaled, y)
                    final_pred = model.predict(X_scaled)
                    models = {"éšæœºæ£®æ—": model}
                    cv_scores = [r2_score(y, final_pred)]
                # 5. è¯„ä¼°
                r2 = r2_score(y, final_pred)
                mae = mean_absolute_error(y, final_pred)
                rmse = np.sqrt(mean_squared_error(y, final_pred))
                cv_mean = np.mean(cv_scores) if cv_scores else r2
                # 6. ç»“æœå±•ç¤º
                st.markdown("### ğŸ“Š é¢„æµ‹ç»“æœ")
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("RÂ² å¾—åˆ†", f"{r2:.3f}")
                col2.metric("å¹³å‡ç»å¯¹è¯¯å·®", f"{mae:.2f}")
                col3.metric("å‡æ–¹æ ¹è¯¯å·®", f"{rmse:.2f}")
                col4.metric("äº¤å‰éªŒè¯å¾—åˆ†", f"{cv_mean:.3f}")

                # 7. å›¾è¡¨
                fig = go.Figure()
                x_axis = features_df[date_col] if date_col else list(range(len(y)))
                fig.add_trace(go.Scatter(x=x_axis, y=y, mode="lines", name="å®é™…å€¼", line=dict(color="#3b82f6", width=3)))
                fig.add_trace(
                    go.Scatter(x=x_axis, y=final_pred, mode="lines", name="é¢„æµ‹å€¼", line=dict(color="#ef4444", width=2, dash="dash"))
                )
                fig.update_layout(title=f"{pred_type} - å®é™…vsé¢„æµ‹", xaxis_title="æ—¥æœŸ", yaxis_title=target_name, height=400)
                st.plotly_chart(fig, use_container_width=True)

                # 8. ç‰¹å¾é‡è¦æ€§
                if show_imp:
                    imp_model = models.get("éšæœºæ£®æ—", list(models.values())[0])
                    if hasattr(imp_model, "feature_importances_"):
                        imp_df = (
                            pd.DataFrame({"feature": feature_cols, "importance": imp_model.feature_importances_})
                            .sort_values("importance", ascending=False)
                            .head(15)
                        )
                        fig_imp = go.Figure(go.Bar(x=imp_df["importance"], y=imp_df["feature"], orientation="h", marker_color="#8b5cf6"))
                        fig_imp.update_layout(title="ç‰¹å¾é‡è¦æ€§æ’å (Top 15)", xaxis_title="é‡è¦æ€§", yaxis_title="ç‰¹å¾", height=500)
                        st.plotly_chart(fig_imp, use_container_width=True)

                # 9. æœªæ¥å¤–æ¨ï¼ˆæ»šåŠ¨é¢„æµ‹ï¼‰
                st.markdown("### ğŸ”® æœªæ¥è¶‹åŠ¿é¢„æµ‹")
                if len(y) >= 10:
                    # ç”¨æœ€å 10 æœŸå¹³å‡æ–œç‡å¤–æ¨
                    recent_slope = np.polyfit(range(10), y.tail(10), 1)[0]
                    future_vals = [y.iloc[-1] + recent_slope * (i + 1) for i in range(forecast_days)]
                    fig_f = go.Figure()
                    fig_f.add_trace(go.Scatter(x=list(range(len(y))), y=y, mode="lines", name="å†å²æ•°æ®", line=dict(color="#3b82f6", width=2)))
                    fig_f.add_trace(
                        go.Scatter(
                            x=list(range(len(y), len(y) + forecast_days)),
                            y=future_vals,
                            mode="lines+markers",
                            name="æœªæ¥é¢„æµ‹",
                            line=dict(color="#10b981", width=3, dash="dot"),
                            marker=dict(size=8),
                        )
                    )
                    fig_f.update_layout(title=f"æœªæ¥{forecast_days}å¤©è¶‹åŠ¿é¢„æµ‹", xaxis_title="æ—¶é—´åºåˆ—", yaxis_title=target_name, height=400)
                    st.plotly_chart(fig_f, use_container_width=True)

                    # 10. æ–‡å­—è§£è¯»
                    trend_direction = "ä¸Šå‡" if recent_slope > 0 else "ä¸‹é™"
                    trend_emoji = "ğŸ“ˆ" if recent_slope > 0 else "ğŸ“‰"
                    sentiment = "ç§¯æ" if recent_slope > 0 else "è°¨æ…"
                    insight = f"""
                    **AIåˆ†ææŠ¥å‘Š:**
                    - **è¶‹åŠ¿åˆ¤æ–­**: å½“å‰å¸‚åœºå‘ˆç°{sentiment}æ€åŠ¿ï¼ŒçŸ­æœŸè¶‹åŠ¿{trend_direction} {trend_emoji}
                    - **é¢„æµ‹ç½®ä¿¡åº¦**: RÂ²å¾—åˆ†{r2:.3f}ï¼Œæ¨¡å‹æ‹Ÿåˆåº¦{'ä¼˜ç§€' if r2 > 0.8 else 'è‰¯å¥½' if r2 > 0.6 else 'ä¸€èˆ¬'}
                    - **é£é™©æç¤º**: é¢„æµ‹åŸºäºå†å²æ•°æ®ï¼Œå®é™…è¡¨ç°å¯èƒ½å—çªå‘äº‹ä»¶å½±å“
                    - **æ“ä½œå»ºè®®**: å»ºè®®ç»“åˆå…¶ä»–æŠ€æœ¯æŒ‡æ ‡å’ŒåŸºæœ¬é¢åˆ†æç»¼åˆåˆ¤æ–­
                    """
                    st.info(insight)

            except Exception as e:
                st.error(f"é¢„æµ‹è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
                st.info("è¯·æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´é¢„æµ‹å‚æ•°")

    with st.expander("â„¹ï¸ AIæ¨¡å‹è¯´æ˜"):
        st.markdown(
            """
        **ä½¿ç”¨çš„AIæŠ€æœ¯:**
        - **é›†æˆå­¦ä¹ **: ç»„åˆå¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹æé«˜é¢„æµ‹ç¨³å®šæ€§
        - **ç‰¹å¾å·¥ç¨‹**: è‡ªåŠ¨ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ã€åŠ¨é‡æŒ‡æ ‡ã€å¸‚åœºæƒ…ç»ªæŒ‡æ ‡
        - **æ—¶é—´åºåˆ—åˆ†æ**: ä¸“é—¨å¤„ç†é‡‘èæ—¶é—´åºåˆ—æ•°æ®çš„ç‰¹æ€§
        - **äº¤å‰éªŒè¯**: ç¡®ä¿æ¨¡å‹åœ¨æœªçŸ¥æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›

        **é¢„æµ‹åŸç†:**
        1. æ•°æ®é¢„å¤„ç† - æ¸…æ´—æ•°æ®ï¼Œå¤„ç†ç¼ºå¤±å€¼
        2. ç‰¹å¾ç”Ÿæˆ - åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡å’Œå¸‚åœºæƒ…ç»ªæŒ‡æ ‡
        3. æ¨¡å‹è®­ç»ƒ - ä½¿ç”¨å†å²æ•°æ®è®­ç»ƒAIæ¨¡å‹
        4. é¢„æµ‹éªŒè¯ - é€šè¿‡äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½
        5. è¶‹åŠ¿é¢„æµ‹ - åŸºäºå­¦ä¹ åˆ°çš„æ¨¡å¼é¢„æµ‹æœªæ¥èµ°åŠ¿
        """
        )


# ------------------------------------------------------------------
# 6. å‘ä¸‹å…¼å®¹æ—§æ¥å£
# ------------------------------------------------------------------
def create_features(df: pd.DataFrame, lookback_days: int = 60) -> pd.DataFrame:
    return create_advanced_features(df, lookback_days)


def train_and_predict(df: pd.DataFrame, target_type: str, lookback_days: int = 60):
    """æ—§æ¥å£ä¿ç•™ï¼Œå†…éƒ¨ç›´æ¥èµ°æ–°é€»è¾‘"""
    show_ai_prediction_dashboard(df)
    return None, None