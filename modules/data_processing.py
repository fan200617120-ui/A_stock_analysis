# modules/data_processing.py (å®Œæ•´ä¿®å¤ç‰ˆ)
import pandas as pd
import numpy as np
from io import BytesIO
from datetime import datetime, timedelta
import streamlit as st

def export_to_excel(df, export_type="å½“å‰è§†å›¾æ•°æ®"):
    """
    å¯¼å‡ºæ•°æ®åˆ°Excelæ–‡ä»¶
    """
    try:
        output = BytesIO()
        
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            # Sheet 1: åŸå§‹æ•°æ®
            df.to_excel(writer, sheet_name='åŸå§‹æ•°æ®', index=False)
            
            # Sheet 2: æŠ€æœ¯æŒ‡æ ‡æ‘˜è¦
            technical_summary = generate_technical_summary(df)
            technical_summary.to_excel(writer, sheet_name='æŠ€æœ¯æŒ‡æ ‡', index=False)
            
            # Sheet 3: å…³é”®æŒ‡æ ‡ç»Ÿè®¡
            key_stats = generate_key_statistics(df)
            key_stats.to_excel(writer, sheet_name='å…³é”®æŒ‡æ ‡', index=True)
            
            # Sheet 4: æœ€è¿‘äº¤æ˜“æ—¥è¯¦æƒ…
            if len(df) > 0:
                latest_data = pd.DataFrame([df.iloc[-1]]).T
                latest_data.columns = ['æœ€æ–°æ•°å€¼']
                latest_data.to_excel(writer, sheet_name='æœ€æ–°æ•°æ®')
        
        output.seek(0)
        return output.getvalue()
        
    except Exception as e:
        st.error(f"å¯¼å‡ºExcelæ—¶å‡ºé”™: {str(e)}")
        return None

def generate_technical_summary(df):
    """ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡æ‘˜è¦"""
    summary_data = []
    
    # åŸºç¡€ç»Ÿè®¡
    if 'å…¨å¤©æ€»é¢' in df.columns:
        summary_data.append({
            'æŒ‡æ ‡': 'æˆäº¤é¢å‡å€¼',
            'æ•°å€¼': f"{df['å…¨å¤©æ€»é¢'].mean():,.0f}",
            'å•ä½': 'äº¿å…ƒ'
        })
    
    if 'åŒ—å‘å‡€å€¼' in df.columns:
        summary_data.append({
            'æŒ‡æ ‡': 'åŒ—å‘èµ„é‡‘å‡å€¼', 
            'æ•°å€¼': f"{df['åŒ—å‘å‡€å€¼'].mean():.2f}",
            'å•ä½': 'äº¿å…ƒ'
        })
    
    if 'å…¨å¤©æ¶¨åœ' in df.columns:
        summary_data.append({
            'æŒ‡æ ‡': 'æ¶¨åœå‡å€¼',
            'æ•°å€¼': f"{df['å…¨å¤©æ¶¨åœ'].mean():.0f}",
            'å•ä½': 'å®¶'
        })
    
    return pd.DataFrame(summary_data)

def generate_key_statistics(df):
    """ç”Ÿæˆå…³é”®æŒ‡æ ‡ç»Ÿè®¡"""
    stats_data = {}
    
    numeric_columns = df.select_dtypes(include=['number']).columns
    
    for col in numeric_columns[:10]:  # é™åˆ¶å‰10ä¸ªæ•°å€¼åˆ—
        if col in df.columns:
            stats_data[col] = {
                'å¹³å‡å€¼': df[col].mean(),
                'æœ€å¤§å€¼': df[col].max(),
                'æœ€å°å€¼': df[col].min(),
                'æ ‡å‡†å·®': df[col].std()
            }
    
    return pd.DataFrame(stats_data).T


# æ•°æ®æ¸…æ´—æ—¥å¿—è®°å½•
cleaning_logs = []

def add_cleaning_log(log_type, count, details):
    """æ·»åŠ æ•°æ®æ¸…æ´—æ—¥å¿—"""
    cleaning_logs.append({
        "æ¸…æ´—ç±»å‹": log_type,
        "æ¶‰åŠæ¡æ•°": count,
        "å…·ä½“å†…å®¹": details,
        "å¤„ç†æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    })

def load_and_clean(uploaded_file):
    """åŠ è½½Excelæ–‡ä»¶å¹¶ä¼˜åŒ–æ—¥æœŸè§£æ"""
    try:
        df = pd.read_excel(uploaded_file, sheet_name=0)
        if 'æ—¥æœŸ' in df.columns:
            # å¼ºåˆ¶è§£ææ—¥æœŸå¹¶ä¿®å¤å¹´ä»½
            original_date_count = len(df)
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
            # ä¿®å¤å¹´ä»½ï¼ˆå°äº2020å¹´çš„åŠ 15å¹´ï¼‰
            mask = df['æ—¥æœŸ'].dt.year < 2020
            corrected_year_count = mask.sum()
            if corrected_year_count > 0:
                df.loc[mask, 'æ—¥æœŸ'] = df.loc[mask, 'æ—¥æœŸ'] + pd.DateOffset(years=15)
                add_cleaning_log(
                    "æ—¥æœŸå¹´ä»½ä¿®å¤", 
                    corrected_year_count, 
                    f"å°†{corrected_year_count}æ¡å¹´ä»½<2020çš„æ—¥æœŸåŠ 15å¹´"
                )
            # è¿‡æ»¤æ— æ•ˆæ—¥æœŸï¼ˆNaTï¼‰
            invalid_dates = df['æ—¥æœŸ'].isna().sum()
            if invalid_dates > 0:
                df = df[df['æ—¥æœŸ'].notna()].copy()
                add_cleaning_log(
                    "æ— æ•ˆæ—¥æœŸè¿‡æ»¤", 
                    invalid_dates, 
                    f"è¿‡æ»¤{invalid_dates}æ¡æ— æ³•è§£æçš„æ— æ•ˆæ—¥æœŸ"
                )
            # ä¿®æ”¹ï¼šæŒ‰æ—¥æœŸå‡åºæ’åˆ—ï¼ˆä»æ—©åˆ°æ™šï¼‰
            df = df.sort_values('æ—¥æœŸ', ascending=True).reset_index(drop=True)
        
        # å¤„ç†ç©ºå€¼
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        null_count = df[numeric_columns].isna().sum().sum()
        if null_count > 0:
            df[numeric_columns] = df[numeric_columns].fillna(0)
            add_cleaning_log(
                "æ•°å€¼ç©ºå€¼å¡«å……", 
                null_count, 
                f"å°†{null_count}ä¸ªæ•°å€¼å‹å­—æ®µç©ºå€¼å¡«å……ä¸º0"
            )
        return df
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        add_cleaning_log("æ–‡ä»¶è¯»å–å¤±è´¥", 0, f"é”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
        return None

def filter_non_trading_days(df):
    """è¿‡æ»¤éäº¤æ˜“æ—¥ï¼ˆå…³é”®æŒ‡æ ‡å…¨ä¸º0çš„æ—¥æœŸï¼‰"""
    if df is None or len(df) == 0:
        return df
    
    # å…³é”®äº¤æ˜“æŒ‡æ ‡åˆ—ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼‰
    key_columns = ['å…¨å¤©æ€»é¢', 'åŒ—å‘å‡€å€¼', 'ä¸Šæ¶¨', 'ä¸‹è·Œ', 'å…¨å¤©æ¶¨åœ']
    existing_keys = [col for col in key_columns if col in df.columns]
    
    if not existing_keys:
        add_cleaning_log("éäº¤æ˜“æ—¥è¿‡æ»¤", 0, "æœªæ‰¾åˆ°å…³é”®äº¤æ˜“æŒ‡æ ‡åˆ—ï¼Œè·³è¿‡è¿‡æ»¤")
        return df
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼šå…³é”®æŒ‡æ ‡ä¸å…¨ä¸º0
    df['is_trading_day'] = df[existing_keys].apply(
        lambda row: not (row == 0).all(), axis=1
    )
    
    # ç»Ÿè®¡å¹¶æ˜¾ç¤ºè¿‡æ»¤çš„éäº¤æ˜“æ—¥æ•°é‡
    non_trading_days = df[~df['is_trading_day']].copy()
    non_trading_count = len(non_trading_days)
    if non_trading_count > 0:
        non_trading_days['æ—¥æœŸ_str'] = non_trading_days['æ—¥æœŸ'].dt.strftime('%Y-%m-%d').fillna('æ— æ•ˆæ—¥æœŸ')
        non_trading_dates = ', '.join(non_trading_days['æ—¥æœŸ_str'].tolist())
        add_cleaning_log(
            "éäº¤æ˜“æ—¥è¿‡æ»¤", 
            non_trading_count, 
            f"è¿‡æ»¤æ—¥æœŸï¼š{non_trading_dates}"
        )
    
    # è¿”å›ä»…åŒ…å«äº¤æ˜“æ—¥çš„æ•°æ®ï¼ˆä¿æŒå‡åºæ’åˆ—ï¼‰
    return df[df['is_trading_day']].drop(columns=['is_trading_day']).copy()

def validate_and_clean_data(df):
    """éªŒè¯æ•°æ®å¹¶ç¡®ä¿ä»Šæ˜¨å·®é¢æ­£ç¡®è®¡ç®— """
    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸå‡åºæ’åˆ—ï¼ˆä»æ—©åˆ°æ™šï¼‰
    df = df.sort_values('æ—¥æœŸ', ascending=True).reset_index(drop=True)
    
    # ä¿®æ­£å°æ¿ç‡
    if 'å…¨å¤©å°æ¿ç‡' in df.columns:
        board_rate = df['å…¨å¤©å°æ¿ç‡']
        if (board_rate > 1).any():
            corrected_count = (board_rate > 1).sum()
            df['å…¨å¤©å°æ¿ç‡'] = df['å…¨å¤©å°æ¿ç‡'] / 100
            add_cleaning_log(
                "å°æ¿ç‡æ•°å€¼ä¿®æ­£", 
                corrected_count, 
                f"å°†{corrected_count}æ¡>1çš„å°æ¿ç‡æ•°å€¼é™¤ä»¥100"
            )
    
    # è¡¥å…¨ä»Šæ˜¨å·®é¢ - å¢å¼ºè®¡ç®—é€»è¾‘
    if 'ä»Šæ˜¨å·®é¢' not in df.columns and 'å…¨å¤©æ€»é¢' in df.columns:
        # æŒ‰æ—¥æœŸå‡åºæ’åˆ—è®¡ç®—å·®é¢ï¼ˆæ­£ç¡®çš„æ—¶åºï¼‰
        df_sorted = df.sort_values('æ—¥æœŸ', ascending=True).copy()
        df_sorted['ä»Šæ˜¨å·®é¢'] = df_sorted['å…¨å¤©æ€»é¢'].diff()
        df = df_sorted.sort_values('æ—¥æœŸ', ascending=True).reset_index(drop=True)
        add_cleaning_log(
            "ä»Šæ˜¨å·®é¢è¡¥å…¨", 
            len(df), 
            "åŸºäº'å…¨å¤©æ€»é¢'å­—æ®µè®¡ç®—å¹¶è¡¥å…¨'ä»Šæ˜¨å·®é¢'åˆ—"
        )
    elif 'ä»Šæ˜¨å·®é¢' in df.columns:
        # ç¡®ä¿ä»Šæ˜¨å·®é¢ä¸ä¸º0ï¼ˆå¦‚æœæ•°æ®å¼‚å¸¸ï¼‰
        zero_count = (df['ä»Šæ˜¨å·®é¢'] == 0).sum()
        if zero_count > len(df) * 0.8:  # å¦‚æœ80%ä»¥ä¸Šçš„æ•°æ®éƒ½æ˜¯0ï¼Œé‡æ–°è®¡ç®—
            df_sorted = df.sort_values('æ—¥æœŸ', ascending=True).copy()
            df_sorted['ä»Šæ˜¨å·®é¢'] = df_sorted['å…¨å¤©æ€»é¢'].diff()
            df = df_sorted.sort_values('æ—¥æœŸ', ascending=True).reset_index(drop=True)
            add_cleaning_log(
                "ä»Šæ˜¨å·®é¢é‡æ–°è®¡ç®—", 
                zero_count, 
                f"é‡æ–°è®¡ç®—{zero_count}æ¡ä¸º0çš„ä»Šæ˜¨å·®é¢æ•°æ®"
            )
    
    # è¡¥å…¨å…¨å¤©æ€»è·Œåœ
    if 'å…¨å¤©è·Œåœ' not in df.columns:
        df['å…¨å¤©è·Œåœ'] = 0
        if 'ä¸»æ¿è·Œåœæ•°' in df.columns:
            df['å…¨å¤©è·Œåœ'] += df['ä¸»æ¿è·Œåœæ•°'].fillna(0)
        if 'åˆ›ä¸šæ¿è·Œåœæ•°' in df.columns:
            df['å…¨å¤©è·Œåœ'] += df['åˆ›ä¸šæ¿è·Œåœæ•°'].fillna(0)
        if 'åŒ—è¯è·Œåœæ•°' in df.columns:
            df['å…¨å¤©è·Œåœ'] += df['åŒ—è¯è·Œåœæ•°'].fillna(0)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè®¡ç®—äº†è·Œåœæ•°æ®
        if (df['å…¨å¤©è·Œåœ'] > 0).any():
            add_cleaning_log(
                "å…¨å¤©è·Œåœè¡¥å…¨", 
                len(df), 
                "åŸºäºå„æ¿å—è·Œåœæ•°è®¡ç®—å…¨å¤©æ€»è·Œåœ"
            )
        else:
            add_cleaning_log(
                "å…¨å¤©è·Œåœåˆå§‹åŒ–", 
                len(df), 
                "åˆå§‹åŒ–å…¨å¤©è·Œåœåˆ—ä¸º0ï¼ˆæ— æ¿å—è·Œåœæ•°æ®ï¼‰"
            )    
   
    
    # ç¡®ä¿æ¶¨åœæ¿å¸‚å€¼åˆ†å¸ƒåˆ—å­˜åœ¨ä¸”ä¸ºæ•°å€¼
    capital_columns = ['æ¶¨åœæ¿>100äº¿', '50äº¿<æ¶¨åœæ¿<100äº¿', '20äº¿<æ¶¨åœæ¿<50äº¿', 'æ¶¨åœæ¿<20äº¿']
    for col in capital_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        else:
            # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¸º0
            df[col] = 0
    
    return df

def filter_data_by_days(df, time_range_str):
    """æ ¹æ®æ—¶é—´èŒƒå›´å­—ç¬¦ä¸²è¿‡æ»¤æ•°æ® - ä¿®å¤ç‰ˆæœ¬"""
    if df is None or len(df) == 0:
        return df
    
    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸå‡åºæ’åˆ—
    df = df.sort_values('æ—¥æœŸ', ascending=True).reset_index(drop=True)
    
    if time_range_str == "å…¨éƒ¨æ•°æ®":
        return df
    
    days_map = {
        "æœ€è¿‘5å¤©": 5,
        "æœ€è¿‘10å¤©": 10, 
        "æœ€è¿‘20å¤©": 20,
        "æœ€è¿‘30å¤©": 30
    }
    
    days = days_map.get(time_range_str, 10)
    
    if len(df) <= days:
        return df
    
    # å–æœ€æ–°çš„æ—¥æœŸï¼ˆæœ€åä¸€è¡Œï¼‰å¹¶å‘å‰è®¡ç®—å¤©æ•°
    latest_date = df['æ—¥æœŸ'].iloc[-1]
    start_date = latest_date - timedelta(days=days-1)
    filtered_df = df[df['æ—¥æœŸ'] >= start_date].copy()
    
    return filtered_df.sort_values('æ—¥æœŸ', ascending=True).reset_index(drop=True)

def safe_get_value(df, column, default=0):
    """å®‰å…¨è·å–æ•°æ®å€¼ - ä¿®å¤ç‰ˆæœ¬"""
    if column not in df.columns:
        return default
    
    # ç¡®ä¿æ•°æ®æ˜¯æŒ‰æ—¥æœŸå‡åºæ’åˆ—çš„ï¼ˆæœ€æ–°çš„åœ¨æœ€åï¼‰
    if len(df) == 0:
        return default
    
    # è·å–æœ€æ–°å€¼ï¼ˆæœ€åä¸€è¡Œï¼Œå› ä¸ºæ•°æ®æ˜¯æŒ‰æ—¥æœŸå‡åºæ’åˆ—çš„ï¼‰
    latest_value = df[column].iloc[-1]
    
    # å¤„ç†ç©ºå€¼
    if pd.isna(latest_value):
        return default
    
    return latest_value

def create_trading_alerts(df):
    """æ™ºèƒ½é¢„è­¦ç³»ç»Ÿ - ä¿®å¤æ—¶é—´è½´ç‰ˆæœ¬"""
    alerts = []
    
    if len(df) < 5:
        return alerts
    
    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æœ€æ–°æ•°æ®ï¼ˆæ•°æ®æ˜¯æŒ‰æ—¥æœŸå‡åºæ’åˆ—çš„ï¼Œæœ€æ–°æ•°æ®åœ¨æœ€åä¸€è¡Œï¼‰
    latest_data = df.iloc[-1]
    
    # æˆäº¤é‡å¼‚å¸¸
    volume_avg = df['å…¨å¤©æ€»é¢'].tail(20).mean()
    current_volume = latest_data['å…¨å¤©æ€»é¢']
    if current_volume > volume_avg * 1.5:
        alerts.append({
            "type": "warning",
            "message": f"ğŸ“ˆ æˆäº¤é‡å¼‚å¸¸æ”¾å¤§: è¾ƒ20æ—¥å‡å€¼å¢åŠ {(current_volume/volume_avg-1)*100:.1f}%",
            "value": f"{current_volume:,.0f} vs å‡å€¼{volume_avg:,.0f}"
        })
    elif current_volume < volume_avg * 0.7:
        alerts.append({
            "type": "info", 
            "message": f"ğŸ“‰ æˆäº¤é‡èç¼©: è¾ƒ20æ—¥å‡å€¼å‡å°‘{(1-current_volume/volume_avg)*100:.1f}%",
            "value": f"{current_volume:,.0f} vs å‡å€¼{volume_avg:,.0f}"
        })
    
    # æ¶¨åœå®¶æ•°å¼‚å¸¸
    if 'å…¨å¤©æ¶¨åœ' in df.columns:
        limit_up_avg = df['å…¨å¤©æ¶¨åœ'].tail(10).mean()
        current_limit_up = latest_data['å…¨å¤©æ¶¨åœ']
        if current_limit_up > limit_up_avg * 2 and limit_up_avg > 0:
            alerts.append({
                "type": "warning",
                "message": f"ğŸ”¥ æ¶¨åœå®¶æ•°å¼‚å¸¸æ´»è·ƒ: è¾¾åˆ°{current_limit_up}å®¶",
                "value": f"è¾ƒ10æ—¥å‡å€¼å¢åŠ {(current_limit_up/limit_up_avg-1)*100:.1f}%"
            })
    
    # åŒ—å‘èµ„é‡‘å¼‚å¸¸
    if 'åŒ—å‘å‡€å€¼' in df.columns:
        north_avg = df['åŒ—å‘å‡€å€¼'].tail(10).mean()
        current_north = latest_data['åŒ—å‘å‡€å€¼']
        if abs(current_north) > abs(north_avg) * 3 and north_avg != 0:
            direction = "æµå…¥" if current_north > 0 else "æµå‡º"
            alerts.append({
                "type": "warning" if abs(current_north) > 100 else "info",
                "message": f"ğŸŒŠ åŒ—å‘èµ„é‡‘å¤§å¹…{direction}: {current_north:+.0f}äº¿",
                "value": f"è¾ƒ10æ—¥å‡å€¼æ”¾å¤§{abs(current_north/north_avg):.1f}å€"
            })
    
    # å°æ¿ç‡å¼‚å¸¸
    if 'å…¨å¤©å°æ¿ç‡' in df.columns:
        board_rate = latest_data['å…¨å¤©å°æ¿ç‡'] * 100
        if board_rate < 50:
            alerts.append({
                "type": "warning",
                "message": f"ğŸ“Š å°æ¿ç‡åä½: {board_rate:.1f}%",
                "value": "å¸‚åœºæ‰“æ¿æƒ…ç»ªè¾ƒå·®"
            })
        elif board_rate > 80:
            alerts.append({
                "type": "success",
                "message": f"ğŸ“Š å°æ¿ç‡ä¼˜ç§€: {board_rate:.1f}%", 
                "value": "å¸‚åœºæ‰“æ¿æƒ…ç»ªé«˜æ¶¨"
            })
    
    return alerts
# modules/data_processing.py (æ·»åŠ æ•°æ®è´¨é‡ç›‘æ§)
def add_data_quality_monitor(df):
    """æ•°æ®è´¨é‡ç›‘æ§é¢æ¿"""
    st.markdown("###  æ•°æ®è´¨é‡")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # æ•°æ®å®Œæ•´ç‡
        completeness = (1 - df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
        st.metric("æ•°æ®å®Œæ•´ç‡", f"{completeness:.1f}%", 
                 delta="ä¼˜ç§€" if completeness > 95 else "è‰¯å¥½" if completeness > 85 else "éœ€å…³æ³¨")
    
    with col2:
        # å…³é”®æŒ‡æ ‡å®Œæ•´ç‡
        key_columns = ['å…¨å¤©æ€»é¢', 'åŒ—å‘å‡€å€¼', 'å…¨å¤©æ¶¨åœ', 'ä¸Šæ¶¨', 'ä¸‹è·Œ']
        available_keys = [col for col in key_columns if col in df.columns]
        if available_keys:
            key_completeness = (1 - df[available_keys].isnull().sum().sum() / (len(df) * len(available_keys))) * 100
            st.metric("å…³é”®æŒ‡æ ‡å®Œæ•´ç‡", f"{key_completeness:.1f}%",
                     delta="ä¼˜ç§€" if key_completeness > 98 else "è‰¯å¥½" if key_completeness > 90 else "éœ€å…³æ³¨")
        else:
            st.metric("å…³é”®æŒ‡æ ‡", "æœªæ‰¾åˆ°")
    
    with col3:
        # æ—¥æœŸè¿ç»­æ€§
        if 'æ—¥æœŸ' in df.columns:
            date_diff = df['æ—¥æœŸ'].diff().dt.days
            gap_days = (date_diff > 1).sum()
            continuity = (1 - gap_days / len(df)) * 100
            st.metric("æ—¥æœŸè¿ç»­æ€§", f"{continuity:.1f}%",
                     delta=f"{gap_days}ä¸ªé—´éš”" if gap_days > 0 else "è¿ç»­")
        else:
            st.metric("æ—¥æœŸè¿ç»­æ€§", "æ— æ—¥æœŸåˆ—")
    
    with col4:
        # æ•°æ®æ—¶æ•ˆæ€§
        if 'æ—¥æœŸ' in df.columns:
            latest_date = df['æ—¥æœŸ'].iloc[-1]
            days_since_update = (pd.Timestamp.now() - latest_date).days
            st.metric("æœ€æ–°æ•°æ®", latest_date.strftime('%m-%d'),
                     delta=f"{days_since_update}å¤©å‰")
        else:
            st.metric("æ•°æ®æ—¶æ•ˆ", "æœªçŸ¥")

def calculate_risk_level(df):
    """è®¡ç®—ç»¼åˆé£é™©ç­‰çº§"""
    if len(df) < 5:
        return "æ•°æ®ä¸è¶³"
    
    risk_score = 0
    latest = df.iloc[-1]
    
    # æˆäº¤é¢é£é™©ï¼ˆç¼©é‡ï¼‰
    if 'å…¨å¤©æ€»é¢' in df.columns:
        volume_ma5 = df['å…¨å¤©æ€»é¢'].tail(5).mean()
        current_volume = latest['å…¨å¤©æ€»é¢']
        if current_volume < volume_ma5 * 0.7:
            risk_score += 2
        elif current_volume < volume_ma5 * 0.8:
            risk_score += 1
    
    # åŒ—å‘èµ„é‡‘é£é™©ï¼ˆå¤§å¹…æµå‡ºï¼‰
    if 'åŒ—å‘å‡€å€¼' in df.columns:
        if latest['åŒ—å‘å‡€å€¼'] < -50:
            risk_score += 2
        elif latest['åŒ—å‘å‡€å€¼'] < -20:
            risk_score += 1
    
    # å¸‚åœºæƒ…ç»ªé£é™©ï¼ˆè·Œåœå¢å¤šï¼‰
    if 'å…¨å¤©è·Œåœ' in df.columns and 'å…¨å¤©æ¶¨åœ' in df.columns:
        if latest['å…¨å¤©è·Œåœ'] > 30:
            risk_score += 2
        elif latest['å…¨å¤©è·Œåœ'] > 20:
            risk_score += 1
        # æ¶¨åœè·Œåœæ¯”
        if latest['å…¨å¤©æ¶¨åœ'] > 0:
            limit_ratio = latest['å…¨å¤©è·Œåœ'] / latest['å…¨å¤©æ¶¨åœ']
            if limit_ratio > 1:
                risk_score += 1
    
    # å°æ¿ç‡é£é™©
    if 'å…¨å¤©å°æ¿ç‡' in df.columns:
        if latest['å…¨å¤©å°æ¿ç‡'] < 0.5:
            risk_score += 1
    
    # ç¡®å®šé£é™©ç­‰çº§
    if risk_score >= 4:
        return "é«˜é£é™©"
    elif risk_score >= 2:
        return "ä¸­é£é™©"
    else:
        return "ä½é£é™©"

# modules/data_processing.py (ä¿®å¤æ³¢åŠ¨ç‡è®¡ç®—)
def calculate_volatility(df, window=20):
    """è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡ - ä¿®å¤ç‰ˆæœ¬"""
    if 'å…¨å¤©æ€»é¢' in df.columns and len(df) >= window:
        # ä½¿ç”¨æˆäº¤é¢çš„å˜åŒ–ç‡æ¥è®¡ç®—æ³¢åŠ¨ç‡
        df_sorted = df.sort_values('æ—¥æœŸ', ascending=True).copy()
        returns = df_sorted['å…¨å¤©æ€»é¢'].pct_change().dropna()
        
        if len(returns) >= window:
            # è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡ï¼ˆä½¿ç”¨æœ€æ–°æ•°æ®ï¼‰
            recent_returns = returns.tail(window)
            volatility = recent_returns.std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
            return float(volatility)
    
    # å¦‚æœæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•ä¼°ç®—
    if 'å…¨å¤©æ€»é¢' in df.columns and len(df) > 1:
        df_sorted = df.sort_values('æ—¥æœŸ', ascending=True).copy()
        returns = df_sorted['å…¨å¤©æ€»é¢'].pct_change().dropna()
        if len(returns) > 0:
            volatility = returns.std() * np.sqrt(252) * (len(returns) / 252)  # è°ƒæ•´åçš„å¹´åŒ–æ³¢åŠ¨ç‡
            return float(volatility)
    
    return None

def calculate_max_drawdown(df):
    """è®¡ç®—æœ€å¤§å›æ’¤ - ä¿®å¤ç‰ˆæœ¬"""
    if 'å…¨å¤©æ€»é¢' in df.columns and len(df) > 1:
        df_sorted = df.sort_values('æ—¥æœŸ', ascending=True).copy()
        
        # è®¡ç®—ç´¯è®¡æ”¶ç›Šç‡ï¼ˆæ¨¡æ‹Ÿä»·æ ¼åºåˆ—ï¼‰
        # å‡è®¾åˆå§‹ä»·æ ¼ä¸º100ï¼Œç”¨æˆäº¤é¢å˜åŒ–ç‡æ¨¡æ‹Ÿä»·æ ¼å˜åŒ–
        initial_price = 100
        returns = df_sorted['å…¨å¤©æ€»é¢'].pct_change().fillna(0)
        cumulative_returns = (1 + returns).cumprod()
        price_series = initial_price * cumulative_returns
        
        # è®¡ç®—å›æ’¤
        peak = price_series.expanding().max()
        drawdown = (price_series - peak) / peak
        max_dd = drawdown.min()
        
        return float(max_dd) if not pd.isna(max_dd) else 0.0
    
    return 0.0

def calculate_risk_level(df):
    """è®¡ç®—ç»¼åˆé£é™©ç­‰çº§ - ä¿®å¤ç‰ˆæœ¬"""
    if len(df) < 5:
        return "æ•°æ®ä¸è¶³"
    
    risk_score = 0
    latest = df.iloc[-1]  # ä½¿ç”¨æœ€åä¸€è¡Œï¼ˆæœ€æ–°æ•°æ®ï¼‰
    
    # 1. æˆäº¤é¢é£é™©ï¼ˆç¼©é‡ï¼‰
    if 'å…¨å¤©æ€»é¢' in df.columns:
        volume_ma5 = df['å…¨å¤©æ€»é¢'].tail(5).mean()
        current_volume = latest['å…¨å¤©æ€»é¢']
        if current_volume < volume_ma5 * 0.7:
            risk_score += 2
            st.sidebar.warning("âš ï¸ æˆäº¤é¢æ˜¾è‘—èç¼©")
        elif current_volume < volume_ma5 * 0.8:
            risk_score += 1
    
    # 2. åŒ—å‘èµ„é‡‘é£é™©ï¼ˆå¤§å¹…æµå‡ºï¼‰
    if 'åŒ—å‘å‡€å€¼' in df.columns:
        current_north = latest['åŒ—å‘å‡€å€¼']
        if current_north < -50:
            risk_score += 2
            st.sidebar.warning("âš ï¸ åŒ—å‘èµ„é‡‘å¤§å¹…æµå‡º")
        elif current_north < -20:
            risk_score += 1
    
    # 3. å¸‚åœºæƒ…ç»ªé£é™©ï¼ˆè·Œåœå¢å¤šï¼‰
    if 'å…¨å¤©è·Œåœ' in df.columns and 'å…¨å¤©æ¶¨åœ' in df.columns:
        current_limit_down = latest['å…¨å¤©è·Œåœ']
        current_limit_up = latest['å…¨å¤©æ¶¨åœ']
        
        if current_limit_down > 30:
            risk_score += 2
            st.sidebar.warning("âš ï¸ è·Œåœå®¶æ•°è¿‡å¤š")
        elif current_limit_down > 20:
            risk_score += 1
        
        # æ¶¨åœè·Œåœæ¯”
        if current_limit_up > 0:
            limit_ratio = current_limit_down / current_limit_up
            if limit_ratio > 1:
                risk_score += 1
                st.sidebar.info("â„¹ï¸ è·Œåœå¤šäºæ¶¨åœ")
    
    # 4. å°æ¿ç‡é£é™©
    if 'å…¨å¤©å°æ¿ç‡' in df.columns:
        current_board_rate = latest['å…¨å¤©å°æ¿ç‡']
        if current_board_rate < 0.5:
            risk_score += 1
            st.sidebar.info("â„¹ï¸ å°æ¿ç‡åä½")
    
    # 5. æ³¢åŠ¨ç‡é£é™©
    volatility = calculate_volatility(df)
    if volatility and volatility > 0.3:  # 30%ä»¥ä¸Šå¹´åŒ–æ³¢åŠ¨ç‡
        risk_score += 1
        st.sidebar.warning("âš ï¸ å¸‚åœºæ³¢åŠ¨ç‡è¾ƒé«˜")
    
    # ç¡®å®šé£é™©ç­‰çº§
    if risk_score >= 4:
        return "é«˜é£é™©"
    elif risk_score >= 2:
        return "ä¸­é£é™©"
    else:
        return "ä½é£é™©"

def get_cleaning_logs():
    """è·å–æ•°æ®æ¸…æ´—æ—¥å¿—"""
    return cleaning_logs